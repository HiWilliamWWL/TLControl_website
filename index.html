<!DOCTYPE html
	PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">

<head>

	<!-- Google tag (gtag.js) -->
	<script async src="https://www.googletagmanager.com/gtag/js?id=G-1K3DMPC9Q9"></script>
	<script>
  		window.dataLayer = window.dataLayer || [];
  		function gtag(){dataLayer.push(arguments);}
  		gtag('js', new Date());

 		gtag('config', 'G-1K3DMPC9Q9');
	</script>
	<title>Globally Consistent Normal Orientation for Point Clouds by Regularizing the Winding-Number Field</title>
	<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
	<link rel="icon" href="./images/QQ图片20220912112705.jpg" sizes="64x64" type="image/png">

	<meta name="robots" content="index,follow">
	<meta name="description"
		content="
		Feature lines are important geometric cues in characterizing the structure of a CAD model. Despite great progress in both explicit reconstruction and implicit reconstruction, it remains a challenging task to reconstruct a polygonal surface equipped with feature lines, especially when the input point cloud is noisy and lacks faithful normal vectors. In this paper, we develop a multistage algorithm, named RFEPS, to address this challenge. The key steps include (1) denoising the point cloud based on the assumption of local planarity, (2) identifying the feature-line zone by optimization of discrete optimal transport, (3) augmenting the point set so that sufficiently many additional points are generated on potential geometry edges, and (4) generating a polygonal surface that interpolates the augmented point set based on restricted power diagram. We demonstrate through extensive experiments that RFEPS, benefiting from the edge-point augmentation and the feature preserving explicit reconstruction, outperforms state of the art methods in terms of the reconstruction quality, especially in terms of the ability to reconstruct missing feature lines.">
		<link rel="author" href="" />


	<!-- Fonts and stuff -->
	<link href='http://fonts.googleapis.com/css?family=Open+Sans:400italic,700italic,800italic,400,700,800'
		rel='stylesheet' type='text/css'>
	<link rel="stylesheet" type="text/css" href="css/project.css" media="screen" />
	<link rel="stylesheet" type="text/css" media="screen" href="css/iconize.css" />
	<script src="js/google-code-prettify/prettify.js"></script>
</head>

<body>
	<div id="content">
		<div id="content-inner">
			<div class="logo-container">
			    <a href="https://www.hku.hk/" target="_blank" class="logo" style="margin-right: 20px;"><img src="./logos/HKU_logo.png" height="40" border="0"></a>
			    <a href="https://www.upenn.edu/" target="_blank" class="logo" style="margin-right: 20px;"><img src="./logos/UPenn_logo.png" height="90" border="0"></a>
			    <a href="https://www.tamu.edu/" target="_blank" class="logo" style="margin-right: 20px;"><img src="./logos/TAMU_logo.png" height="30" border="0"></a>
			</div>


			<div class="section head">

				<h1>TLcontrol: Trajectory and Language Control for Human Motion Synthesis</h1>

				<div class="authors">
					<a href="https://hiwilliamwwl.github.io/WWL_webpage/" target="_blank">Weilin Wan</a><sup> 1, 2</sup>&#160;&#160;
					<a href="https://frank-zy-dou.github.io/" target="_blank">Zhiyang Dou</a><sup> 1,2</sup>&#160;&#160;
					<a href="https://www.cs.hku.hk/index.php/people/academic-staff/taku">Taku Komura</a><sup> 1</sup>&#160;&#160;
					<a href="https://engineering.tamu.edu/cse/profiles/Wang-Wenping.html">Wenping Wang</a><sup> 3</sup>&#160;&#160;
					<a href="https://www.seas.upenn.edu/~dineshj/">Dinesh Jayaraman</a><sup> 2</sup>&#160;&#160;
					<a href="https://lingjie0206.github.io/">Lingjie Liu</a><sup> 2</sup>&#160;&#160;
				</div>

				<div class="affiliations">
					<sup>1</sup><a href="https://www.hku.hk/" target="_blank">The University of Hong Kong</a>&#160;&#160;
					<sup>2</sup><a href="https://www.upenn.edu/" target="_blank">University of Pennsylvania</a>&#160;&#160;
					<sup>3</sup><a href="https://www.tamu.edu/" target="_blank">Texas A&M University</a>&#160;&#160;
				</div>
				<!--<div class="venue"> ACM Transactions on Graphics(SIGGRAPH 2023) </div> 
				<div class="venue"> Selected for inclusion in the Technical Papers Video Trailer. </div>
				<div class="venue" style="color:#e53333"> SIGGRAPH 2023 Technical Papers Best Paper Award. </div>>-->
			</div>
			<div class="section downloads">
			    <center>
			        <ul style="padding-left: 0; list-style: none;">
			            <li class="grid" style="display: inline-block; margin: 0 35px;">
			                <div class="griditem" style="text-align: center;">
			                    <a href="https://arxiv.org/"><img src="images/pdf.png"></a><br/>
			                    <a href="https://arxiv.org/">Paper</a>
			                </div>
			            </li>
			            <li class="grid" style="display: inline-block; margin: 0 0px;">
			                <div class="griditem" style="text-align: center;">
			                    <a href="https://www.youtube.com/watch?v=thN1sQwP8R4"><img src="images/video.png"></a><br/>
			                    <a href="https://www.youtube.com/watch?v=thN1sQwP8R4">Video</a>
			                </div>
			            </li>
			            <li class="grid" style="display: inline-block; margin: 0 0px;">
			                <div class="griditem" style="text-align: center;">
			                    <a href="https://github.com/"><img src="images/data_ico.png"></a><br/>
			                    <a href="https://github.com/">Code <br/>(coming soon)</a>
			                </div>
			            </li>
			        </ul>
			    </center>
			</div>


				





			<div class="section abstract">
				<h2>Abstract</h2><br>
				<div class="row" style="margin-bottom:10px">
					<div class="col" style="text-align:center">
						<img class="thumbnail" src="figs/TLControl_Teaser.png" style="width:90%; margin-bottom:20px">
					</div>

				</div>
				<p>
					Controllable human motion synthesis is essential for applications in AR/VR, gaming, movies, and embodied AI. Existing methods often focus solely on either language or full trajectory control, lacking precision in synthesizing motions aligned with user-specified trajectories, especially for multi-joint control. To address these issues, we present TLControl, a new method for realistic human motion synthesis, incorporating both low-level trajectory and high-level language semantics controls. Specifically, we first train a VQ-VAE to learn a compact latent motion space organized by body parts. We then propose a Masked Trajectories Transformer to make coarse initial predictions of full trajectories of joints based on the learned latent motion space, with user-specified partial trajectories and text descriptions as conditioning. Finally, we introduce an efficient test-time optimization to refine these coarse predictions for accurate trajectory control. Experiments demonstrate that TLControl outperforms the state-of-the-art in trajectory accuracy and time efficiency, making it practical for interactive and high-quality animation generation.			</p>
			</div>

			<!--<div class="section abstract">
				<h2>Full Video</h2><br>
				<center>
					<!-- <iframe width="640" height="360" src="data/video.mp4" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe> -->
					<!--<iframe width="640" height="360" src="https://www.youtube.com/embed/RFqPwH7QFEI" frameborder="0"
						allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"
						allowfullscreen></iframe>-->
					<!--iframe src="./data/video.mp4" allow="autoplay; encrypted-media" webkitallowfullscreen="" mozallowfullscreen="" allowfullscreen="" width="560" height="315" frameborder="0"></iframe-->
					<!--<p style="font-size:11px; text-align:center">
					Download Video: <a href="data/video.mp4" target="_blank">HD</a> (MP4, 111 MB)
				</p>
				</center>
			</div>-->

			<div class="section abstract">
				<h2>Demo Video</h2>

				<center>
					<div class="row" style="margin-bottom:5px">
					<div class="col" style="text-align:center">
						<p align="center">
							<iframe width="800" height="450" src="https://www.youtube.com/embed/thN1sQwP8R4?si=ReLXj_TMT0PPR9Wq" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>
						</p>

<!--						<img class="thumbnail" src="figs/gallery.png" style="width:98%; margin-bottom:20px">-->
				</center>
			</div>

			<br>
			
			

			
		<div class="section list">
				<h2>Citation</h2>
				<div class="section bibtex">
					<pre>@article{
}</div>
			</div>
			
		<!-- 	<div class="section list">
				<h2>Related Links</h2>
				<div class="row" style="margin-top:15px">
				<li>Parts of <a href="https://github.com/Totoro97/NeuS" target="_blank">our PyTorch implementation</a> are taken from <a href="https://github.com/lioryariv/idr" target="_blank">IDR</a> and <a href="https://github.com/yenchenlin/nerf-pytorch" target="_blank">NeRF-pytorch</a>.
				<li>Check the concurrent works of learning neural implicit surfaces: </br> 			
				&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp<a href="https://arxiv.org/abs/2104.10078" target="_blank">UNISURF: Unifying Neural Implicit Surfaces and Radiance Fields for Multi-View Reconstruction</a>, Oechsle et al. 2021 </br>
				&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp<a href="https://arxiv.org/abs/2106.12052" target="_blank">Volume Rendering of Neural Implicit Surfaces</a>, Yariv et al. 2021 
				<li>Also check other works about neural scene representations and neural rendering from our group: </br> 
				&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp<a href="https://lingjie0206.github.io/papers/NSVF/" target="_blank">Neural Sparse Voxel Fields:</a>, Liu et al. 2020 </br> 	
				&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp<a href="https://gvv.mpi-inf.mpg.de/projects/nonrigid_nerf/" target="_blank">Non-Rigid Neural Radiance Fields: Reconstruction and Novel View Synthesis of a Dynamic Scene From Monocular Video</a>, Tretschk et al. 2021</br> 	
				&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp<a href="http://gvv.mpi-inf.mpg.de/projects/NeuralActor/" target="_blank">Neural Actor: Neural Free-view Synthesis of Human Actors with Pose Control</a>, Liu et al. 2021 </br> 	
				&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp<a href="https://liuyuan-pal.github.io/NeuRay/" target="_blank">Neural Rays for Occlusion-aware Image-based Rendering</a>, Liu et al. 2021. </br> 	
				</div>
			</div>
			
			<div class="section list">
				<h2>Acknowledgements</h2>
				<div class="row" style="margin-top:15px">
				<p>We thank Michael Oechsle for providing the results of UNISURF. Christian Theobalt was supported by ERC Consolidator Grant 770784. Lingjie Liu was supported by Lise Meitner Postdoctoral Fellowship.</p> 
				</div>
			</div> -->
			
			
			<div class="section">
				<hr class="smooth">
			Page last updated 
				<script type="text/javascript">
					var m = "This page was last updated: " + document.lastModified;
					var p = m.length - 9;
					document.writeln("<left>");
					document.write(m.substring(p, 0) + ".");
					document.writeln("</left>");
				</script>
			
			</div>
		</div>
	</div>
</body>
</html>
